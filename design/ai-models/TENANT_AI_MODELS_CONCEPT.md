# Tenant AI Models ‚Äî Solution Design & Implementation Plan

## 1. Feature Overview

**Ziel:** Tenants k√∂nnen LLM- und Embedding-Modelle registrieren und f√ºr verschiedene Zwecke (z.B. Conversation Title Generation, Description Generation, Trace Analysis) nutzen. Mehrere Modelle pro Purpose Group erm√∂glichen Load-Balancing. Die Integration erfolgt √ºber direkte LLM-API-Calls im Agent-Service (Phase 1), sp√§ter LangChain als Abstraktionsschicht (Phase 2+).

**Use-Cases f√ºr v0.1.0:**
1. **Conversation Title Generation** ‚Äî Nach der ersten AI-Antwort wird automatisch ein KI-generierter Titel gepatcht
2. **Smart Description Generation** ‚Äî "Generate with AI" Button auf allen Entities mit Description-Feld
3. **Trace Analysis** ‚Äî "Analyze Error" bei fehlgeschlagenen Traces + "Summarize with AI" (Short/Medium/Long) bei allen Traces

---

## 2. Design-Entscheidungen & Empfehlungen

### Was ich anders designen w√ºrde als dein initialer Entwurf:

#### 2.1 Provider-Enum statt nur Credential-Type
> Du hast `credentials.Type: TENANT_AI_MODEL` vorgeschlagen. Ich empfehle stattdessen einen **eigenen `provider`-Feld** auf `tenant_ai_models`, denn:
> - Credentials sind generisch und mehrere AI Models k√∂nnen dasselbe Credential teilen (z.B. gleicher Azure OpenAI Account, aber verschiedene Deployments/Modelle)
> - Der Credential-Type `TENANT_AI_MODEL` sagt nichts √ºber den konkreten Provider aus
> - Besser: `credentials.type = "AI_MODEL_PROVIDER"` als neuer generischer Type, und `tenant_ai_models.provider` bestimmt den konkreten Provider (AZURE_OPENAI, OPENAI, ANTHROPIC, etc.)

#### 2.2 Config als JSON statt vieler Spalten
> Verschiedene Provider brauchen verschiedene Config-Parameter (Azure braucht endpoint+deployment, OpenAI nur model_name, Anthropic nochmal anders). Daher: **`config: JSON`** Feld mit einem `TenantAIModelConfigValidator` der pro Provider die richtige Config validiert ‚Äî genau wie bei Chat Agents.

#### 2.3 Purpose Groups flexibler gestalten
> Statt fest auf 2 Purpose Groups zu beschr√§nken, empfehle ich ein JSON-Array-Feld `purpose_groups: list[str]`. Ein Modell kann f√ºr mehrere Zwecke taugen.

#### 2.4 Load-Balancing √ºber `priority` + `is_active`
> Statt komplexer Load-Balancing-Logik: Jedes Model bekommt ein `priority: int` Feld. Der Agent-Service w√§hlt Round-Robin aus aktiven Models mit gleicher Priority, oder Fallback auf niedrigere Priority bei Fehler.

#### 2.5 Kein RBAC pro Model ‚Äî Tenant-Rollen stattdessen
> AI Models sind tenant-weite Konfiguration und brauchen **keine eigene Members-Tabelle**. Stattdessen:
> - `GLOBAL_ADMIN` kann alles managen
> - `TENANT_AI_MODELS_ADMIN` (neue Rolle) kann AI Models auf Tenant-Ebene verwalten
> - Alle Tenant-Mitglieder mit `READER`-Rolle k√∂nnen AI Models sehen (lesen)
> - Das vereinfacht das Modell erheblich ‚Äî keine `tenant_ai_model_members` Tabelle n√∂tig

#### 2.6 Alle LLM-Calls √ºber Agent-Service
> Description Generation, Trace Analysis, Title Generation ‚Äî alles l√§uft √ºber den Agent-Service. Der Platform-Service liefert nur Config, der Agent-Service f√ºhrt aus. Frontend ruft Agent-Service Endpoints auf.

#### 2.7 "Test Model" Button in der Konfiguration
> Beim Erstellen/Editieren eines AI Models gibt es einen **"Test Model"** Button. Dieser sendet eine einfache Ping-Message an das LLM (z.B. `"Reply with: OK"`) und pr√ºft, ob die Config + Credential funktionieren. Erfolg ‚Üí gr√ºner Toast. Fehler ‚Üí Fehlermeldung im Dialog. Der Test l√§uft √ºber den Agent-Service (`POST /ai/test-model`).

#### 2.8 Tracing-Daten als TOML statt JSON
> Wenn Trace-Objekte an den LLM gesendet werden (f√ºr Analyze Error und Summarize), werden sie vorher in **TOML-Format** konvertiert. TOML ist deutlich kompakter als JSON (keine Klammern, weniger Anf√ºhrungszeichen) und **spart signifikant Tokens** bei gro√üen Trace-Objekten.

#### 2.9 "Analyze Error" als Dialog statt Slide-Over
> Im Gegensatz zum "Summarize with AI" (das ein Slide-Over Panel nutzt) √∂ffnet **"Analyze Error" einen einfachen Dialog (Modal)** mit der AI-Antwort. Das ist angemessener, da Error-Analyse typischerweise einen einzelnen Node betrifft und keine parallele Navigation erfordert.

---

## 3. Unterst√ºtzte LangChain Provider & Config

### 3.1 LLM Chat Models (Phase 1 ‚Äî Top-Priorit√§t)

| Provider | LangChain Package | Config-Felder |
|----------|------------------|---------------|
| **Azure OpenAI** | `langchain-openai` (`AzureChatOpenAI`) | `endpoint`, `api_version`, `deployment_name`, `api_key` (via credential) |
| **OpenAI** | `langchain-openai` (`ChatOpenAI`) | `model_name`, `api_key` (via credential), `organization?`, `base_url?` |
| **Anthropic** | `langchain-anthropic` (`ChatAnthropic`) | `model_name`, `api_key` (via credential) |
| **Google (Gemini)** | `langchain-google-genai` (`ChatGoogleGenerativeAI`) | `model_name`, `api_key` (via credential) |
| **Ollama** (Self-hosted) | `langchain-ollama` (`ChatOllama`) | `model_name`, `base_url` |

### 3.2 LLM Chat Models (Phase 2 ‚Äî Nice-to-have)

| Provider | LangChain Package | Config-Felder |
|----------|------------------|---------------|
| **Mistral AI** | `langchain-mistralai` | `model_name`, `api_key` |
| **Groq** | `langchain-groq` | `model_name`, `api_key` |
| **AWS Bedrock** | `langchain-aws` | `model_id`, `region`, `credentials_profile` |
| **Deepseek** | `langchain-deepseek` | `model_name`, `api_key` |
| **Cohere** | `langchain-cohere` | `model_name`, `api_key` |

### 3.3 Embedding Models (Phase 1)

| Provider | LangChain Package | Config-Felder |
|----------|------------------|---------------|
| **Azure OpenAI** | `langchain-openai` (`AzureOpenAIEmbeddings`) | `endpoint`, `api_version`, `deployment_name`, `api_key` |
| **OpenAI** | `langchain-openai` (`OpenAIEmbeddings`) | `model_name`, `api_key` |
| **Ollama** | `langchain-ollama` (`OllamaEmbeddings`) | `model_name`, `base_url` |

### 3.4 Credential-Config pro Provider

```
AI_MODEL_PROVIDER Credential secret_value (JSON):
‚îú‚îÄ‚îÄ AZURE_OPENAI:   { "api_key": "..." }
‚îú‚îÄ‚îÄ OPENAI:         { "api_key": "..." }
‚îú‚îÄ‚îÄ ANTHROPIC:      { "api_key": "..." }
‚îú‚îÄ‚îÄ GOOGLE_GENAI:   { "api_key": "..." }
‚îú‚îÄ‚îÄ OLLAMA:         {}  (kein API Key n√∂tig, self-hosted)
‚îú‚îÄ‚îÄ MISTRAL:        { "api_key": "..." }
‚îî‚îÄ‚îÄ GROQ:           { "api_key": "..." }
```

---

## 4. Entity Design

### 4.1 `tenant_ai_models` (Neue Tabelle)

```
tenant_ai_models
‚îú‚îÄ‚îÄ id: str (UUID, PK)
‚îú‚îÄ‚îÄ tenant_id: str (FK ‚Üí tenants.id, CASCADE)
‚îú‚îÄ‚îÄ name: str (max 255) ‚Äî Display-Name, z.B. "GPT-4o Production"
‚îú‚îÄ‚îÄ description: str (optional, max 2000)
‚îú‚îÄ‚îÄ type: enum [LLM_MODEL, EMBEDDING_MODEL]
‚îú‚îÄ‚îÄ provider: enum [AZURE_OPENAI, OPENAI, ANTHROPIC, GOOGLE_GENAI, OLLAMA, MISTRAL, GROQ]
‚îú‚îÄ‚îÄ purpose_groups: JSON/Array [CONVERSATION_TITLE_GENERATION, DESCRIPTION_GENERATION, ...]
‚îú‚îÄ‚îÄ config: JSON ‚Äî Provider-spezifische Config (OHNE Secrets!)
‚îÇ   ‚îú‚îÄ‚îÄ Azure OpenAI: { "endpoint": "https://...", "api_version": "2024-12-01-preview", "deployment_name": "gpt-4o" }
‚îÇ   ‚îú‚îÄ‚îÄ OpenAI:       { "model_name": "gpt-4o", "organization": "org-...", "base_url": null }
‚îÇ   ‚îú‚îÄ‚îÄ Anthropic:    { "model_name": "claude-sonnet-4-20250514" }
‚îÇ   ‚îú‚îÄ‚îÄ Ollama:       { "model_name": "llama3", "base_url": "http://localhost:11434" }
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ credential_id: str (FK ‚Üí credentials.id, SET NULL, optional) ‚Äî Referenz auf Credential mit API Key
‚îú‚îÄ‚îÄ priority: int (default 0) ‚Äî F√ºr Load-Balancing/Fallback (0 = h√∂chste Priorit√§t)
‚îú‚îÄ‚îÄ is_active: bool (default false)
‚îî‚îÄ‚îÄ created_at, updated_at, created_by, updated_by (AuditMixin)
```

> **Kein `tenant_ai_model_members`!** Zugriffssteuerung l√§uft √ºber Tenant-Rollen:
> - `GLOBAL_ADMIN` / `TENANT_AI_MODELS_ADMIN` ‚Üí CRUD auf alle AI Models
> - `READER` (alle Tenant-Mitglieder) ‚Üí K√∂nnen AI Models sehen

### 4.2 Credential Erweiterung

```
CredentialTypeEnum (erweitert):
‚îú‚îÄ‚îÄ API_KEY (bestehend)
‚îú‚îÄ‚îÄ BASIC_AUTH (bestehend)
‚îú‚îÄ‚îÄ OPENAPI_CONNECTION (bestehend)
‚îî‚îÄ‚îÄ AI_MODEL_PROVIDER (NEU) ‚Äî { "api_key": "sk-..." }
```

### 4.3 Neue Enums

```python
class AIModelTypeEnum(str, Enum):
    LLM_MODEL = "LLM_MODEL"
    EMBEDDING_MODEL = "EMBEDDING_MODEL"

class AIModelProviderEnum(str, Enum):
    AZURE_OPENAI = "AZURE_OPENAI"
    OPENAI = "OPENAI"
    ANTHROPIC = "ANTHROPIC"
    GOOGLE_GENAI = "GOOGLE_GENAI"
    OLLAMA = "OLLAMA"
    MISTRAL = "MISTRAL"
    GROQ = "GROQ"

class AIModelPurposeGroupEnum(str, Enum):
    CONVERSATION_TITLE_GENERATION = "CONVERSATION_TITLE_GENERATION"
    CONVERSATION_SUMMARIZATION = "CONVERSATION_SUMMARIZATION"
    DESCRIPTION_GENERATION = "DESCRIPTION_GENERATION"
    TRACE_ANALYSIS = "TRACE_ANALYSIS"
    GENERAL = "GENERAL"
```

### 4.4 Neue Tenant-Rolle

```python
class TenantRolesEnum(str, Enum):
    # ... bestehende Rollen ...
    TENANT_AI_MODELS_ADMIN = "TENANT_AI_MODELS_ADMIN"
```

---

## 5. API Routes

### 5.1 Platform-Service ‚Äî Tenant AI Models CRUD

Prefix: `/api/v1/platform-service/tenants/{tenant_id}/ai-models`

| Method | Path | Beschreibung | Permissions |
|--------|------|-------------|------------|
| GET | `/ai-models` | Liste AI Models | `authenticate()` ‚Äî alle Tenant-Mitglieder |
| POST | `/ai-models` | Neues AI Model anlegen | `GLOBAL_ADMIN`, `TENANT_AI_MODELS_ADMIN` |
| GET | `/ai-models/{id}` | AI Model Detail | `authenticate()` ‚Äî alle Tenant-Mitglieder |
| PATCH | `/ai-models/{id}` | AI Model updaten | `GLOBAL_ADMIN`, `TENANT_AI_MODELS_ADMIN` |
| DELETE | `/ai-models/{id}` | AI Model l√∂schen | `GLOBAL_ADMIN`, `TENANT_AI_MODELS_ADMIN` |

> **Keine `/principals` Endpoints** ‚Äî Zugriff l√§uft komplett √ºber Tenant-Rollen.

### 5.2 Platform-Service ‚Äî AI Models Config Endpoint (Service-to-Service)

| Method | Path | Beschreibung | Auth |
|--------|------|-------------|------|
| GET | `/ai-models/by-purpose?purpose_group=...&type=LLM_MODEL` | Aktive Models nach Purpose | `X-Service-Key` + Bearer |

> Gibt Models inkl. entschl√ºsselter Credential-Secrets zur√ºck (Service-to-Service only!).

### 5.3 Platform-Service ‚Äî Conversation PATCH (bestehend, kein Change n√∂tig)

| Method | Path | Beschreibung |
|--------|------|-------------|
| PATCH | `/conversations/{id}` | Name updaten (wird vom Agent-Service aufgerufen) |

### 5.4 Agent-Service ‚Äî AI Feature Endpoints (NEU)

Prefix: `/api/v1/agent-service/tenants/{tenant_id}/ai`

| Method | Path | Beschreibung | Auth |
|--------|------|-------------|------|
| POST | `/ai/generate-description` | Description generieren/verbessern | Bearer |
| POST | `/ai/analyze-trace` | Trace Fehler analysieren | Bearer |
| POST | `/ai/summarize-trace` | Trace zusammenfassen (Short/Medium/Long) | Bearer |
| GET | `/ai/capabilities` | Verf√ºgbare AI-Features f√ºr diesen Tenant | Bearer |
| POST | `/ai/test-model` | Model-Config testen (Ping an LLM) | Bearer |

#### 5.4.1 `POST /ai/generate-description`

```json
// Request
{
  "entity_type": "chat_agent",       // chat_agent | autonomous_agent | tool | credential | chat_widget | ai_model
  "entity_name": "My N8N Agent",
  "existing_description": "n8n bot",  // Optional ‚Äî roh-description die poliert werden soll
  "context": {                         // Optional ‚Äî zus√§tzlicher Kontext
    "type": "N8N",
    "config": { ... }
  }
}

// Response
{
  "description": "An N8N-based conversational AI agent for automated customer support workflows with integrated ticket management."
}
```

#### 5.4.2 `POST /ai/analyze-trace`

```json
// Request
{
  "trace_id": "uuid",
  "node_id": "uuid",                   // Optionaler spezifischer Node
  "error": "Connection refused...",
  "node_name": "HTTP Request",
  "node_type": "http",
  "input": { ... },
  "output": null
}

// Response
{
  "analysis": "## Error Analysis\n\n**Root Cause:** The HTTP endpoint at `https://api.example.com/v2/users` is unreachable...\n\n**Suggested Fix:**\n1. Verify the endpoint URL...\n2. Check network connectivity..."
}
```

#### 5.4.3 `POST /ai/summarize-trace`

```json
// Request
{
  "trace_id": "uuid",
  "detail_level": "short",            // short | medium | long
  "nodes": [                           // Trace-Nodes zur Analyse
    { "name": "Agent", "type": "agent", "status": "completed", "duration": 2.3, ... },
    { "name": "Tool Call", "type": "tool", "status": "completed", ... }
  ]
}

// Response
{
  "summary": "## Trace Summary\n\n**Status:** Completed in 4.2s\n**Flow:** User query ‚Üí Agent reasoning ‚Üí Tool call (web search) ‚Üí Response generation\n\n**Key observations:** ..."
}
```

#### 5.4.4 `POST /ai/test-model`

```json
// Request
{
  "provider": "AZURE_OPENAI",
  "config": {
    "endpoint": "https://my-resource.openai.azure.com/",
    "api_version": "2024-12-01-preview",
    "deployment_name": "gpt-4o"
  },
  "credential_id": "uuid"        // Optional ‚Äî existing credential
}

// Response (Success)
{ "success": true, "message": "Model responded successfully", "response_time_ms": 423 }

// Response (Failure)
{ "success": false, "message": "Authentication failed: Invalid API key", "response_time_ms": 0 }
```

#### 5.4.5 `GET /ai/capabilities`

```json
// Response ‚Äî Frontend nutzt dies um AI-Buttons ein/auszublenden
{
  "title_generation": true,            // Hat aktive LLM Models mit CONVERSATION_TITLE_GENERATION
  "description_generation": true,      // Hat aktive LLM Models mit DESCRIPTION_GENERATION
  "trace_analysis": true,              // Hat aktive LLM Models mit TRACE_ANALYSIS
  "summarization": false               // Keine Models f√ºr CONVERSATION_SUMMARIZATION
}
```

> **Wichtig:** Das Frontend ruft `GET /ai/capabilities` beim Laden und cached das Ergebnis im Context. AI-Buttons werden **nur** angezeigt wenn die jeweilige Capability `true` ist.

### 5.5 Title Generation ‚Äî Interner Flow (kein Public Endpoint)

```
User sendet 1. Nachricht
  ‚Üí Agent-Service erstellt Conversation (via FE)
  ‚Üí Agent antwortet (Streaming)
  ‚Üí Nach STREAM_END: Agent-Service pr√ºft ob 1. Nachricht
  ‚Üí Wenn ja + AI Models konfiguriert:
    ‚Üí Ruft Platform-Service GET /ai-models/by-purpose?purpose_group=CONVERSATION_TITLE_GENERATION ab
    ‚Üí Baut LLM Client
    ‚Üí Generiert Titel
    ‚Üí PATCHt Conversation-Name via Platform-Service
    ‚Üí Sendet SSE Event: CONVERSATION_TITLE_UPDATE
```

### 5.6 SSE Event Types (NEU)

```typescript
SSEStreamMessageType.CONVERSATION_TITLE_UPDATE = "CONVERSATION_TITLE_UPDATE"

// Event payload
{
  type: "CONVERSATION_TITLE_UPDATE",
  config: {
    conversationId: "uuid",
    title: "KI-generierter Titel"
  }
}
```

---

## 6. Architektur-Diagramm

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Agent-Service   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Platform-Service    ‚îÇ
‚îÇ  (React/TS)  ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   (Go/Gin)        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  (Python/FastAPI)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò SSE  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò HTTP ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                       ‚îÇ                         ‚îÇ
      ‚îÇ POST /ai/*            ‚îÇ Direct LLM Calls        ‚îÇ PostgreSQL
      ‚îÇ                       ‚ñº                         ‚ñº
      ‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
      ‚îÇ              ‚îÇ  LLM Provider ‚îÇ          ‚îÇ  tenant_ai_models ‚îÇ
      ‚îÇ              ‚îÇ  (Azure/OAI/ ‚îÇ          ‚îÇ  credentials      ‚îÇ
      ‚îÇ              ‚îÇ   Anthropic)  ‚îÇ          ‚îÇ  conversations    ‚îÇ
      ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
      ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ  AI Capabilities Context (FE)          ‚îÇ
  ‚îÇ  ‚Üí Steuert Sichtbarkeit aller         ‚îÇ
  ‚îÇ    AI-Buttons basierend auf            ‚îÇ
  ‚îÇ    GET /ai/capabilities                ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Use-Case Flows:

```
Title Generation (automatisch, nach 1. Antwort):
  FE ‚Üí POST /messages ‚Üí Agent-Service ‚Üí Streaming ‚Üí STREAM_END
  ‚Üí async: Agent-Service ‚Üí GET /ai-models/by-purpose ‚Üí LLM ‚Üí PATCH /conversations ‚Üí SSE Event

Description Generation (User-triggered):
  FE ‚Üí POST /ai/generate-description ‚Üí Agent-Service ‚Üí GET /ai-models/by-purpose ‚Üí LLM ‚Üí Response
  FE ‚Üê { description: "..." }

Trace Error Analysis (User-triggered):
  FE ‚Üí POST /ai/analyze-trace ‚Üí Agent-Service ‚Üí GET /ai-models/by-purpose ‚Üí LLM ‚Üí Response
  FE ‚Üê { analysis: "## Error Analysis\n..." }

Trace Summarization (User-triggered):
  FE ‚Üí POST /ai/summarize-trace ‚Üí Agent-Service ‚Üí GET /ai-models/by-purpose ‚Üí LLM ‚Üí Response
  FE ‚Üê { summary: "## Trace Summary\n..." }
```

---

## 7. Frontend UI Design

### 7.1 Tenant Settings > AI Models

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Tenant Settings > AI Models                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  [+ Add AI Model]    [Search...]    [Filter: Type ‚ñæ]    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üü¢ GPT-4o Production        Azure OpenAI | LLM   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Purpose: Title Gen, Description Gen, General    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Priority: 0                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üü¢ Claude Sonnet             Anthropic | LLM      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Purpose: Trace Analysis                         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Priority: 0                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ üî¥ text-embedding-3-large    OpenAI | Embedding   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Purpose: General                                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Priority: 0                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7.2 AI Model Dialog (Create/Edit)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Create AI Model                    [X] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                         ‚îÇ
‚îÇ  Name:     [________________________]   ‚îÇ
‚îÇ  Description: [____________________]    ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Type:     [LLM Model          ‚ñæ]       ‚îÇ
‚îÇ  Provider: [Azure OpenAI       ‚ñæ]       ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  ‚îÄ‚îÄ Provider Configuration ‚îÄ‚îÄ           ‚îÇ
‚îÇ  Endpoint:        [https://...    ]     ‚îÇ
‚îÇ  API Version:     [2024-12-01-pre ‚ñæ]    ‚îÇ
‚îÇ  Deployment Name: [gpt-4o         ]     ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Credential: [Select Credential  ‚ñæ]    ‚îÇ
‚îÇ              [+ Create new]             ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Purpose Groups: [‚òë Title Gen]          ‚îÇ
‚îÇ                  [‚òê Summarization]      ‚îÇ
‚îÇ                  [‚òë Description Gen]    ‚îÇ
‚îÇ                  [‚òë Trace Analysis]     ‚îÇ
‚îÇ                  [‚òë General]            ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  Priority: [0]                          ‚îÇ
‚îÇ  Active:   [Toggle ON/OFF]              ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  [üîå Test Model]                        ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ           [Cancel]  [Create]            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 7.3 "Generate with AI" Button (Description Generation)

Platzierung: Neben dem Description-Feld in Create/Edit Dialogen aller Entities.

```
  Description: [My raw notes about th]  [‚ú®] ‚Üê "Generate with AI" Tooltip
                                          ‚îÇ
                                          ‚ñº onClick:
                              POST /ai/generate-description
                              {
                                entity_type: "chat_agent",
                                entity_name: "My App",
                                existing_description: "My raw notes about th..."
                              }
                              ‚Üí Ergebnis wird ins Description-Feld geschrieben
```

**Verhalten:**
- Button ist ein kleines AI-Stern-Icon (‚ú®) mit Tooltip "Generate with AI"
- Nur sichtbar wenn `capabilities.description_generation === true`
- Wenn `existing_description` vorhanden ‚Üí LLM poliert die Roh-Description auf
- Wenn leer ‚Üí LLM generiert basierend auf `entity_name` + `context`
- W√§hrend Loading: Spinner im Icon, Description-Feld disabled
- Ergebnis wird ins Feld gesetzt, User kann noch editieren vor Speichern

**Betroffene Entities:**
- Chat Agents (Create/Edit Dialog)
- Autonomous Agents (Create/Edit Dialog)
- Tools (Create/Edit Dialog)
- Credentials (Create/Edit Dialog)
- Chat Widgets (Create/Edit Dialog)
- AI Models (Create/Edit Dialog)

### 7.4 Trace Analysis ‚Äî Slide-Over Panel

#### 7.4.1 Fehlerhafte Traces: "Analyze Error" Button

In der Traces-Liste: Bei Traces/Nodes mit `status: FAILED` wird ein "Analyze Error" Text-Button/Link in der Zeile angezeigt.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Traces                                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úÖ Agent Run          2.3s    completed                    ‚îÇ
‚îÇ  ‚úÖ Tool: Web Search   0.8s    completed                    ‚îÇ
‚îÇ  ‚ùå HTTP Request        ‚Äî      failed     [Analyze Error]   ‚îÇ
‚îÇ  ‚úÖ Response Gen        1.1s   completed                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

Click auf "Analyze Error" ‚Üí √ñffnet einen **Dialog (Modal)** mit der AI-Analyse.

#### 7.4.2 Trace Detail: "Summarize with AI" Split-Button

Wenn man einen Trace √∂ffnet (Trace-Detail-View), gibt es oben rechts einen Split-Button:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Trace: Agent Run #abc123                                    ‚îÇ
‚îÇ                                          [‚ú® Summarize ‚ñæ]    ‚îÇ
‚îÇ                                           ‚îú‚îÄ Short (default) ‚îÇ
‚îÇ                                           ‚îú‚îÄ Medium          ‚îÇ
‚îÇ                                           ‚îî‚îÄ Long            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Nodes: ...                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Split-Button Design:**
- Linker Teil: "‚ú® Summarize" ‚Üí Klick = Short (Default)
- Rechter Teil: Dropdown-Pfeil (‚ñæ) ‚Üí √ñffnet Men√º mit Short / Medium / Long
- Nur sichtbar wenn `capabilities.trace_analysis === true`

#### 7.4.3 Analyze Error Dialog

"Analyze Error" √∂ffnet einen **Dialog (Modal)** mit der AI-Analyse:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  AI Error Analysis                      [X] ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  ## Error Analysis                          ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  **Root Cause:**                            ‚îÇ
‚îÇ  The HTTP endpoint at `api.example.com`     ‚îÇ
‚îÇ  returned a 503 Service Unavailable...      ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  **Suggested Fixes:**                       ‚îÇ
‚îÇ  1. Check if the target service is running  ‚îÇ
‚îÇ  2. Verify the URL is correct               ‚îÇ
‚îÇ  3. Check for rate limiting                 ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ---                                        ‚îÇ
‚îÇ  _Generated with GPT-4o_                    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [Re-generate]  [Copy]          [Close]     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 7.4.4 AI Summarize Slide-Over Panel

"Summarize with AI" √∂ffnet ein Slide-Over Panel von rechts:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ‚îÇ  AI Analysis                 [X] ‚îÇ
‚îÇ   Trace Detail     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
‚îÇ   (bestehend)      ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îÇ  ## Error Analysis               ‚îÇ
‚îÇ                    ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îÇ  **Root Cause:**                  ‚îÇ
‚îÇ                    ‚îÇ  The HTTP endpoint at             ‚îÇ
‚îÇ                    ‚îÇ  `api.example.com` returned       ‚îÇ
‚îÇ                    ‚îÇ  a 503 Service Unavailable...     ‚îÇ
‚îÇ                    ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îÇ  **Suggested Fixes:**             ‚îÇ
‚îÇ                    ‚îÇ  1. Check if the target service   ‚îÇ
‚îÇ                    ‚îÇ     is running                    ‚îÇ
‚îÇ                    ‚îÇ  2. Verify the URL is correct     ‚îÇ
‚îÇ                    ‚îÇ  3. Check for rate limiting       ‚îÇ
‚îÇ                    ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îÇ  ---                              ‚îÇ
‚îÇ                    ‚îÇ  _Generated with GPT-4o_          ‚îÇ
‚îÇ                    ‚îÇ                                  ‚îÇ
‚îÇ                    ‚îÇ  [Re-generate] [Copy]             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Panel Features:**
- Slide-Over von rechts (√§hnlich wie Tracing-Sidebar, ~400px breit)
- Markdown-Rendering f√ºr die AI-Antwort
- Loading-State: Skeleton/Spinner w√§hrend LLM generiert
- Footer: "Re-generate" Button + "Copy to Clipboard" Button
- Kleiner Hinweis welches Model verwendet wurde: "_Generated with GPT-4o_"
- Panel bleibt offen, auch wenn man in der Trace-Hierarchie navigiert
- Schlie√üen: X-Button oder Escape

> **Zukunft:** Dieses Panel kann zu einem vollwertigen AI-Chat-Panel erweitert werden, wo man Follow-up-Fragen stellen kann ("Explain step 3 in more detail", "How do I fix the rate limit?").

---

## 8. Schema-Definitionen

### 8.1 Request Schemas (Platform-Service)

```python
class CreateTenantAIModelRequest(BaseModel):
    name: str                                    # max 255
    description: Optional[str] = None            # max 2000
    type: AIModelTypeEnum                        # LLM_MODEL | EMBEDDING_MODEL
    provider: AIModelProviderEnum                # AZURE_OPENAI | OPENAI | ...
    purpose_groups: list[AIModelPurposeGroupEnum] # [CONVERSATION_TITLE_GENERATION, ...]
    config: dict                                 # Provider-specific, validated
    credential_id: Optional[str] = None          # FK to credentials
    priority: int = 0                            # 0 = highest
    is_active: bool = False

class UpdateTenantAIModelRequest(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    purpose_groups: Optional[list[AIModelPurposeGroupEnum]] = None
    config: Optional[dict] = None
    credential_id: Optional[str] = None
    priority: Optional[int] = None
    is_active: Optional[bool] = None
```

### 8.2 Response Schemas (Platform-Service)

```python
class TenantAIModelResponse(BaseModel):
    id: str
    tenant_id: str
    name: str
    description: Optional[str]
    type: AIModelTypeEnum
    provider: AIModelProviderEnum
    purpose_groups: list[AIModelPurposeGroupEnum]
    config: dict
    credential_id: Optional[str]
    priority: int
    is_active: bool
    created_at: datetime
    updated_at: datetime
    created_by: Optional[str]
    updated_by: Optional[str]

class TenantAIModelListResponse(BaseModel):
    items: list[TenantAIModelResponse]
    total: int
```

### 8.3 Service-to-Service Response (mit Secrets)

```python
class AIModelWithSecretResponse(BaseModel):
    """Internal response - includes decrypted credential for agent-service."""
    id: str
    type: AIModelTypeEnum
    provider: AIModelProviderEnum
    config: dict
    credential_secret: Optional[dict]  # Decrypted { "api_key": "..." }
    priority: int
```

---

## 9. Config Validation Examples

### Azure OpenAI (LLM)
```json
{
  "endpoint": "https://my-resource.openai.azure.com/",
  "api_version": "2024-12-01-preview",
  "deployment_name": "gpt-4o"
}
```

### OpenAI (LLM)
```json
{
  "model_name": "gpt-4o",
  "organization": "org-xxx",
  "base_url": null
}
```

### Anthropic (LLM)
```json
{
  "model_name": "claude-sonnet-4-20250514"
}
```

### Google Gemini (LLM)
```json
{
  "model_name": "gemini-2.0-flash"
}
```

### Ollama (LLM, self-hosted)
```json
{
  "model_name": "llama3.1",
  "base_url": "http://localhost:11434"
}
```

---

## 10. Prompts

### 10.1 Title Generation

```
System: You are a conversation title generator. Generate a concise, descriptive title
(maximum 50 characters) for the following conversation. The title should capture the
main topic. Return ONLY the title, nothing else. No quotes, no prefix.

User: {user_first_message}
Assistant: {assistant_first_response_snippet_500_chars}
```

### 10.2 Description Generation (mit bestehender Roh-Description)

```
System: You are a technical writer. Improve the following raw description into a
clear, professional, concise description (1-2 sentences, max 200 characters).
Keep the original intent. Return ONLY the improved description, nothing else.

Entity type: {entity_type}
Entity name: {entity_name}
Raw description: {existing_description}
```

### 10.3 Description Generation (ohne bestehende Description)

```
System: You are a technical writer. Generate a clear, professional, concise description
(1-2 sentences, max 200 characters) for the following entity. Return ONLY the
description, nothing else.

Entity type: {entity_type}
Entity name: {entity_name}
Additional context: {json_context}
```

### 10.4 Trace Error Analysis

```
System: You are a DevOps and AI agent debugging expert. Analyze the following trace
error and provide:
1. Root cause analysis
2. Suggested fixes (actionable steps)
3. Prevention tips

Format your response in Markdown. Be concise but thorough.

Node: {node_name} (type: {node_type})
Status: FAILED
Error: {error_message}
Input (TOML):
{input_toml}
Output (TOML):
{output_toml}
```

### 10.5 Trace Summarization

```
System: You are a trace analysis expert. Summarize the following agent execution trace.
Detail level: {short|medium|long}

For "short": 2-3 sentences covering status, total duration, and main flow.
For "medium": Include each major step, durations, and notable observations.
For "long": Detailed analysis of every node, data flow, performance, and recommendations.

Format in Markdown.

Trace nodes (TOML):
{nodes_toml}
```

---

## 11. Risiken & Offene Punkte

| # | Thema | Risiko/Frage |
|---|-------|-------------|
| 1 | **LLM Libraries in Go** | Go hat kein natives LangChain. F√ºr Phase 1 reichen direkte API-Calls (`go-openai`, Provider-spezifische SDKs), f√ºr Phase 2+ braucht es einen Python-Sidecar. |
| 2 | **Secrets im Transit** | Service-to-Service Endpoint gibt entschl√ºsselte API Keys zur√ºck. TLS-Verschl√ºsselung + starker Service-Key sicherstellen. |
| 3 | **Rate Limiting** | LLM API Calls kosten Geld. Title-Generation nur 1x pro Conversation. Description Gen: Button muss Debounce haben. |
| 4 | **Latency** | Title-Generation async (fire-and-forget Goroutine). Description/Trace-Analyse synchron, aber mit Loading-State im FE. |
| 5 | **Fallback** | Wenn keine AI Models konfiguriert ‚Üí Verhalten wie bisher. AI-Buttons unsichtbar. Kein Breaking Change. |
| 6 | **Multi-Tenant Isolation** | AI Model Configs sind tenant-scoped. GET /ai/capabilities pr√ºft nur Models des eigenen Tenants. |
| 7 | **AI-Button Visibility** | FE cached `GET /ai/capabilities` im AICapabilitiesContext. Cache wird invalidiert bei √Ñnderungen in AI Settings. |

---

## 12. Detaillierter Implementierungsplan

### Phase 1: Platform-Service ‚Äî Entity & CRUD (~2-3 Tage)

| # | Task | Files |
|---|------|-------|
| 1.1 | Neue Enums: `AIModelTypeEnum`, `AIModelProviderEnum`, `AIModelPurposeGroupEnum` | `core/database/enums.py` |
| 1.2 | Neue TenantRole: `TENANT_AI_MODELS_ADMIN` | `core/database/enums.py` |
| 1.3 | `CredentialTypeEnum` erweitern: `AI_MODEL_PROVIDER` | `handlers/validators/credential_validator.py` |
| 1.4 | Credential Validator: Validation f√ºr `AI_MODEL_PROVIDER` secret format | `handlers/validators/credential_validator.py` |
| 1.5 | DB Model: `TenantAIModel` (OHNE Members-Tabelle!) | `core/database/models.py` |
| 1.6 | Alembic Migration | `alembic/versions/` |
| 1.7 | Schemas: Request/Response Models | `schema/requests/tenant_ai_models.py`, `schema/responses/tenant_ai_models.py` |
| 1.8 | Exceptions | `exc/tenant_ai_models.py` |
| 1.9 | Config Validator: `TenantAIModelConfigValidator` (pro Provider) | `handlers/validators/tenant_ai_model_validator.py` |
| 1.10 | Handler: `TenantAIModelHandler` (Tenant-Rolle statt Resource-Permission) | `handlers/tenant_ai_models.py` |
| 1.11 | Dependency: `get_tenant_ai_model_handler` | `handlers/dependencies/tenant_ai_models.py` |
| 1.12 | Routes: CRUD (ohne /principals!) | `apis/v1/tenant_ai_models.py` |
| 1.13 | Route Registration in `app.py` | `app.py` |
| 1.14 | Service-to-Service Endpoint: `GET /ai-models/by-purpose` | `apis/v1/tenant_ai_models.py` |
| 1.15 | Tests: CRUD + Tenant-Role checks | `tests/test_tenant_ai_models.py` |

### Phase 2: Agent-Service ‚Äî AI Feature Endpoints (~3-4 Tage)

| # | Task | Files |
|---|------|-------|
| 2.1 | Platform-Client: `GetAIModelsByPurpose()`, `UpdateConversation()` | `services/platform/client.go` |
| 2.2 | AI Service: LLM Client Factory (Provider-basiert) | `services/ai/llm_client.go` |
| 2.3 | AI Service: Title Generator | `services/ai/title_generator.go` |
| 2.4 | AI Service: Description Generator | `services/ai/description_generator.go` |
| 2.5 | AI Service: Trace Analyzer (mit TOML-Konvertierung) | `services/ai/trace_analyzer.go` |
| 2.6 | AI Handler: `/ai/*` Endpoints + Capabilities + Test Model | `api/handlers/ai.go` |
| 2.7 | Route Registration | `api/routes/routes.go` |
| 2.8 | SSE Event: `CONVERSATION_TITLE_UPDATE` | `api/sse/writer.go` |
| 2.9 | Integration: Title Gen in `SendMessage` Flow | `api/handlers/messages.go` |
| 2.10 | Tests | `tests/unit/` |

### Phase 3: Frontend ‚Äî AI Settings + AI Features (~3-4 Tage)

| # | Task | Files |
|---|------|-------|
| 3.1 | Types: AI Model Enums, Request/Response Types | `api/types.ts` |
| 3.2 | API Client: AI Model CRUD + AI Feature methods | `api/client.ts` |
| 3.3 | AICapabilitiesContext: Cacht `/ai/capabilities` | `contexts/AICapabilitiesContext.tsx` |
| 3.4 | TenantPermissionEnum: `TENANT_AI_MODELS_ADMIN` hinzuf√ºgen | `api/types.ts` |
| 3.5 | Tenant Settings > AI Models Page (Liste + CRUD) | `pages/TenantSettingsPage/AIModelsPage/` |
| 3.6 | AI Model Dialog (Create/Edit, Provider-dynamisch) | `components/dialogs/AIModelDialog/` |
| 3.7 | GenerateWithAIButton Komponente (‚ú® Icon-Button) | `components/common/GenerateWithAIButton/` |
| 3.8 | GenerateWithAIButton in alle Entity-Dialoge einbauen | Diverse Dialoge |
| 3.9 | Credential Dialog: `AI_MODEL_PROVIDER` Type Support | Credential Dialog |
| 3.10 | SSE Handler: `CONVERSATION_TITLE_UPDATE` Event | `pages/ConversationsPage/ConversationsPage.tsx` |
| 3.11 | Sidebar Titel-Update bei Empfang des Events | `ChatSidebar.tsx`, `GlobalChatSidebar.tsx` |
| 3.12 | TraceAnalysisPanel: Slide-Over Komponente f√ºr Summarize | `components/tracing/TraceAnalysisPanel/` |
| 3.13 | AnalyzeErrorDialog: Modal f√ºr Error-Analyse | `components/dialogs/AnalyzeErrorDialog/` |
| 3.14 | "Analyze Error" Button in Trace-Liste (bei FAILED) | Tracing-Komponenten |
| 3.15 | "Summarize with AI" Split-Button in Trace-Detail | Tracing-Komponenten |
| 3.16 | "Test Model" Button in AI Model Dialog | `components/dialogs/AIModelDialog/` |
| 3.17 | Markdown-Renderer f√ºr AI-Antworten | `components/common/MarkdownRenderer/` (falls nicht vorhanden) |

---

## 13. Zusammenfassung der √Ñnderungen pro Service

### Platform-Service
- **Neue Dateien (~7):** model, migration, schemas (2), handler, dependency, routes, exceptions, validator
- **Ge√§nderte Dateien (~3):** `enums.py`, `models.py`, `credential_validator.py`, `app.py`
- **Tests (~1-2):** `test_tenant_ai_models.py`
- **Kein RBAC / Members** ‚Äî deutlich weniger Code als Standard-Entity

### Agent-Service
- **Neue Dateien (~6):** `services/ai/llm_client.go`, `services/ai/title_generator.go`, `services/ai/description_generator.go`, `services/ai/trace_analyzer.go`, `api/handlers/ai.go`
- **Ge√§nderte Dateien (~4):** `services/platform/client.go`, `services/platform/types.go`, `api/handlers/messages.go`, `api/sse/writer.go`, `api/routes/routes.go`
- **Tests (~2-3):** AI service tests

### Frontend-Service
- **Neue Dateien (~8):** AI Models Page, AI Model Dialog, GenerateWithAIButton, TraceAnalysisPanel, AICapabilitiesContext, MarkdownRenderer
- **Ge√§nderte Dateien (~6+):** `types.ts`, `client.ts`, `ConversationsPage.tsx`, `ChatSidebar.tsx`, `GlobalChatSidebar.tsx`, diverse Entity-Dialoge

---

## 14. Zeitsch√§tzung

| Phase | Aufwand | Beschreibung |
|-------|---------|-------------|
| Phase 1: Platform CRUD | 2-3 Tage | Entity (ohne RBAC!), Handler, Routes, Tests |
| Phase 2: Agent-Service AI Endpoints | 3-4 Tage | LLM Client, Title Gen, Description Gen, Trace Analysis, SSE |
| Phase 3: Frontend | 3-4 Tage | Settings Page, AI Buttons, Slide-Over Panel, SSE Handler |
| **Gesamt** | **8-11 Tage** | |
